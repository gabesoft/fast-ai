{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gabe/work/fast-ai/nbs'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  .end_space {\n",
       "      min-height: 1000px;\n",
       "  }\n",
       "  .container {\n",
       "      width: 100%;\n",
       "  }\n",
       "</style"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  .end_space {\n",
    "      min-height: 1000px;\n",
    "  }\n",
    "  .container {\n",
    "      width: 100%;\n",
    "  }\n",
    "</style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/statefarm'\n",
    "SAMPLE_DIR = os.path.join(DATA_DIR, 'sample')\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils.statefarm\n",
    "reload(utils.statefarm)\n",
    "\n",
    "import utils.trainhelper\n",
    "reload(utils.trainhelper)\n",
    "\n",
    "from utils.statefarm import *\n",
    "from utils.trainhelper import get_batches, save_model, read_model, get_classes\n",
    "from utils.utils import save_array, load_array\n",
    "\n",
    "from models.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21601 images belonging to 10 classes.\n",
      "Found 823 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n",
      "['c0/img_82409.jpg', 'c0/img_95245.jpg', 'c0/img_88538.jpg', 'c0/img_14492.jpg', 'c0/img_81194.jpg'] filenames\n",
      "10 classes\n",
      "(224, 224, 3) image shape\n"
     ]
    }
   ],
   "source": [
    "data_dir = DATA_DIR\n",
    "\n",
    "width_igen = image.ImageDataGenerator(width_shift_range=0.1)\n",
    "height_igen = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "shear_igen = image.ImageDataGenerator(shear_range=0.1)\n",
    "rotation_igen = image.ImageDataGenerator(rotation_range=15)\n",
    "channel_igen = image.ImageDataGenerator(channel_shift_range=20)\n",
    "igen = image.ImageDataGenerator(rotation_range=15, \n",
    "                                height_shift_range=0.05, \n",
    "                                shear_range=0.1, \n",
    "                                channel_shift_range=20, \n",
    "                                width_shift_range=0.1)\n",
    "\n",
    "ngen = image.ImageDataGenerator(rescale=1./255) # data normalization\n",
    "\n",
    "train_batches = get_batches(os.path.join(data_dir, 'train'), ngen, batch_size=BATCH_SIZE, shuffle=False)\n",
    "valid_batches = get_batches(os.path.join(data_dir, 'valid'), ngen, batch_size=BATCH_SIZE * 2, shuffle=False)\n",
    "test_batches = get_batches(os.path.join(data_dir, 'test'), ngen, batch_size=BATCH_SIZE, shuffle=False, class_mode=None)\n",
    "\n",
    "train_steps = int(np.ceil(train_batches.samples / BATCH_SIZE))\n",
    "valid_steps = int(np.ceil(valid_batches.samples / (BATCH_SIZE * 2)))\n",
    "test_steps = int(np.ceil(test_batches.samples / BATCH_SIZE))\n",
    "\n",
    "print(train_batches.filenames[:5], 'filenames')\n",
    "print(train_batches.num_class, 'classes')\n",
    "print(train_batches.image_shape, 'image shape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the convolutional layers of VGG16 to generate outputs to the next model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False, pooling=None, input_shape=(224,224,3))\n",
    "\n",
    "train_vgg_preds = vgg.predict_generator(train_batches, train_steps)\n",
    "valid_vgg_preds = vgg.predict_generator(valid_batches, valid_steps)\n",
    "test_vgg_preds = vgg.predict_generator(test_batches, test_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21601, 7, 7, 512) train_vgg_preds.shape\n",
      "(823, 7, 7, 512) valid_vgg_preds.shape\n",
      "(79726, 7, 7, 512) test_vgg_preds.shape\n",
      "(7, 7, 512)\n"
     ]
    }
   ],
   "source": [
    "print(train_vgg_preds.shape, 'train_vgg_preds.shape')\n",
    "print(valid_vgg_preds.shape, 'valid_vgg_preds.shape')\n",
    "print(test_vgg_preds.shape, 'test_vgg_preds.shape')\n",
    "print(vgg.layers[-1].output_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CACHE_DIR = os.path.join(data_dir, 'cache')\n",
    "\n",
    "save_array(os.path.join(CACHE_DIR, 'train_vgg_preds.dat'), train_vgg_preds)\n",
    "save_array(os.path.join(CACHE_DIR, 'valid_vgg_preds.dat'), valid_vgg_preds)\n",
    "save_array(os.path.join(CACHE_DIR, 'test_vgg_preds.dat'), test_vgg_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21601 images belonging to 10 classes.\n",
      "Found 823 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "   train_classes,\n",
    "   valid_classes,\n",
    "   train_labels,\n",
    "   valid_labels,\n",
    "   train_filenames,\n",
    "   valid_filenames,\n",
    "   test_filenames\n",
    ") = get_classes(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21601\n",
      "823\n",
      "21601\n",
      "823\n"
     ]
    }
   ],
   "source": [
    "print(len(train_classes))\n",
    "print(len(valid_classes))\n",
    "print(len(train_labels))\n",
    "print(len(valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21601 samples, validate on 823 samples\n",
      "Epoch 1/10\n",
      "21601/21601 [==============================] - 8s - loss: 6.6340 - acc: 0.1025 - val_loss: 2.4416 - val_acc: 0.0984\n",
      "Epoch 2/10\n",
      "21601/21601 [==============================] - 7s - loss: 6.0400 - acc: 0.1029 - val_loss: 2.3630 - val_acc: 0.1628\n",
      "Epoch 3/10\n",
      "21601/21601 [==============================] - 7s - loss: 5.5270 - acc: 0.1102 - val_loss: 2.2667 - val_acc: 0.1920\n",
      "Epoch 4/10\n",
      "21601/21601 [==============================] - 7s - loss: 5.0928 - acc: 0.1115 - val_loss: 2.1954 - val_acc: 0.2321\n",
      "Epoch 5/10\n",
      "21601/21601 [==============================] - 7s - loss: 4.6161 - acc: 0.1168 - val_loss: 2.1791 - val_acc: 0.35720.\n",
      "Epoch 6/10\n",
      "21601/21601 [==============================] - 7s - loss: 4.2166 - acc: 0.1187 - val_loss: 2.1323 - val_acc: 0.3524\n",
      "Epoch 7/10\n",
      "21601/21601 [==============================] - 7s - loss: 3.8400 - acc: 0.1206 - val_loss: 2.1165 - val_acc: 0.4508\n",
      "Epoch 8/10\n",
      "21601/21601 [==============================] - 7s - loss: 3.4960 - acc: 0.1307 - val_loss: 2.0968 - val_acc: 0.4751\n",
      "Epoch 9/10\n",
      "21601/21601 [==============================] - 7s - loss: 3.1843 - acc: 0.1401 - val_loss: 2.0687 - val_acc: 0.4180\n",
      "Epoch 10/10\n",
      "21601/21601 [==============================] - 7s - loss: 2.9201 - acc: 0.1437 - val_loss: 2.0361 - val_acc: 0.5067\n",
      "Train on 21601 samples, validate on 823 samples\n",
      "Epoch 1/10\n",
      "21601/21601 [==============================] - 7s - loss: 2.6858 - acc: 0.1620 - val_loss: 1.9969 - val_acc: 0.5298\n",
      "Epoch 2/10\n",
      "21601/21601 [==============================] - 7s - loss: 2.4959 - acc: 0.1808 - val_loss: 1.9540 - val_acc: 0.4872\n",
      "Epoch 3/10\n",
      "21601/21601 [==============================] - 7s - loss: 2.3409 - acc: 0.1952 - val_loss: 1.9111 - val_acc: 0.5176\n",
      "Epoch 4/10\n",
      "21601/21601 [==============================] - 7s - loss: 2.2123 - acc: 0.2136 - val_loss: 1.8684 - val_acc: 0.5213\n",
      "Epoch 5/10\n",
      "21601/21601 [==============================] - 7s - loss: 2.0892 - acc: 0.2444 - val_loss: 1.8302 - val_acc: 0.5383\n",
      "Epoch 6/10\n",
      "21601/21601 [==============================] - 7s - loss: 1.9792 - acc: 0.2744 - val_loss: 1.7720 - val_acc: 0.5674\n",
      "Epoch 7/10\n",
      "21601/21601 [==============================] - 7s - loss: 1.8959 - acc: 0.2999 - val_loss: 1.7239 - val_acc: 0.5055\n",
      "Epoch 8/10\n",
      "21601/21601 [==============================] - 7s - loss: 1.8078 - acc: 0.3262 - val_loss: 1.6436 - val_acc: 0.5358\n",
      "Epoch 9/10\n",
      "21601/21601 [==============================] - 7s - loss: 1.7252 - acc: 0.3570 - val_loss: 1.5778 - val_acc: 0.5419\n",
      "Epoch 10/10\n",
      "21601/21601 [==============================] - 7s - loss: 1.6332 - acc: 0.3911 - val_loss: 1.4994 - val_acc: 0.56380.39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f481265c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DROPOUT_RATE = 0.9\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=vgg.layers[-1].output_shape[1:]),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    \n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    \n",
    "    Dense(1024, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    \n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_vgg_preds, train_labels, batch_size=BATCH_SIZE, epochs=10, validation_data=(valid_vgg_preds, valid_labels))\n",
    "\n",
    "model.optimizer.lr = 0.001\n",
    "model.fit(train_vgg_preds, train_labels, batch_size=BATCH_SIZE, epochs=10, validation_data=(valid_vgg_preds, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21601 samples, validate on 823 samples\n",
      "Epoch 1/20\n",
      "21601/21601 [==============================] - 7s - loss: 1.5528 - acc: 0.4217 - val_loss: 1.4364 - val_acc: 0.5529\n",
      "Epoch 2/20\n",
      "21601/21601 [==============================] - 7s - loss: 1.4862 - acc: 0.4497 - val_loss: 1.3377 - val_acc: 0.6087\n",
      "Epoch 3/20\n",
      "21601/21601 [==============================] - 7s - loss: 1.4011 - acc: 0.4818 - val_loss: 1.2734 - val_acc: 0.6416\n",
      "Epoch 4/20\n",
      "21601/21601 [==============================] - 7s - loss: 1.3355 - acc: 0.5053 - val_loss: 1.2261 - val_acc: 0.6488\n",
      "Epoch 5/20\n",
      "21601/21601 [==============================] - 7s - loss: 1.2614 - acc: 0.5360 - val_loss: 1.1615 - val_acc: 0.6586\n",
      "Epoch 6/20\n",
      "21601/21601 [==============================] - 7s - loss: 1.2133 - acc: 0.5566 - val_loss: 1.1002 - val_acc: 0.67800. - ETA: 1s - lo\n",
      "Epoch 7/20\n",
      "21601/21601 [==============================] - 7s - loss: 1.1566 - acc: 0.5793 - val_loss: 1.0569 - val_acc: 0.6926\n",
      "Epoch 8/20\n",
      "21601/21601 [==============================] - 7s - loss: 1.1081 - acc: 0.5968 - val_loss: 1.0228 - val_acc: 0.6926\n",
      "Epoch 9/20\n",
      "21601/21601 [==============================] - 7s - loss: 1.0728 - acc: 0.6096 - val_loss: 0.9855 - val_acc: 0.6950\n",
      "Epoch 10/20\n",
      "21601/21601 [==============================] - 7s - loss: 1.0253 - acc: 0.6340 - val_loss: 0.9680 - val_acc: 0.7011\n",
      "Epoch 11/20\n",
      "21601/21601 [==============================] - 7s - loss: 0.9849 - acc: 0.6492 - val_loss: 0.9398 - val_acc: 0.6974\n",
      "Epoch 12/20\n",
      "21601/21601 [==============================] - 7s - loss: 0.9706 - acc: 0.6525 - val_loss: 0.9317 - val_acc: 0.6914\n",
      "Epoch 13/20\n",
      "21601/21601 [==============================] - 7s - loss: 0.9302 - acc: 0.6704 - val_loss: 0.9088 - val_acc: 0.6962\n",
      "Epoch 14/20\n",
      "21601/21601 [==============================] - 7s - loss: 0.8995 - acc: 0.6814 - val_loss: 0.9112 - val_acc: 0.6780\n",
      "Epoch 15/20\n",
      "21601/21601 [==============================] - 7s - loss: 0.8755 - acc: 0.6932 - val_loss: 0.9030 - val_acc: 0.6987\n",
      "Epoch 16/20\n",
      "21601/21601 [==============================] - 7s - loss: 0.8516 - acc: 0.7002 - val_loss: 0.9176 - val_acc: 0.6865\n",
      "Epoch 17/20\n",
      "21601/21601 [==============================] - 7s - loss: 0.8208 - acc: 0.7109 - val_loss: 0.9173 - val_acc: 0.6865\n",
      "Epoch 18/20\n",
      "21601/21601 [==============================] - 7s - loss: 0.7958 - acc: 0.7203 - val_loss: 0.9165 - val_acc: 0.6792\n",
      "Epoch 19/20\n",
      "21601/21601 [==============================] - 7s - loss: 0.7859 - acc: 0.7288 - val_loss: 0.9513 - val_acc: 0.6659\n",
      "Epoch 20/20\n",
      "21601/21601 [==============================] - 7s - loss: 0.7632 - acc: 0.7327 - val_loss: 0.9630 - val_acc: 0.6549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5f7966e390>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "model.fit(train_vgg_preds, train_labels, batch_size=BATCH_SIZE, epochs=20, validation_data=(valid_vgg_preds, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 6,493,194\n",
      "Trainable params: 6,492,170\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model(data_dir, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_vgg_preds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected flatten_22_input to have shape (None, 7, 7, 512) but got array with shape (64, 224, 224, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-e579727d0ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1192\u001b[0m                                             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m                                             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m                                             verbose=verbose)\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   2288\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2290\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2291\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1817\u001b[0m         \"\"\"\n\u001b[1;32m   1818\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[0;32m-> 1819\u001b[0;31m                                     self._feed_input_shapes)\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    142\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected flatten_22_input to have shape (None, 7, 7, 512) but got array with shape (64, 224, 224, 3)"
     ]
    }
   ],
   "source": [
    "test_dir = os.path.join(data_dir, 'test')\n",
    "test_batches = get_batches(test_dir, shuffle=False, batch_size=BATCH_SIZE, class_mode=None)\n",
    "test_steps = int(np.ceil(test_batches.samples / BATCH_SIZE))\n",
    "test_preds = model.predict_generator(test_batches, test_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_preds = model.predict(valid_vgg_preds, batch_size=BATCH_SIZE*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 823 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_dir = os.path.join(data_dir, 'valid')\n",
    "valid_batches = get_batches(valid_dir, batch_size=BATCH_SIZE * 2, shuffle=False)\n",
    "valid_steps = int(np.ceil(valid_batches.samples / (BATCH_SIZE * 2)))\n",
    "valid_preds = model.predict_generator(valid_batches, valid_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_dir = os.path.join(data_dir, 'results')\n",
    "save_array(os.path.join(results_dir, 'predictions'), test_preds)\n",
    "save_array(os.path.join(results_dir, 'filenames'), test_batches.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import backend as K\n",
    "\n",
    "def onehot(x):\n",
    "    to_categorical(x, 10)\n",
    "    \n",
    "def do_clip(arr, mx):\n",
    "    return np.clip(arr, (1 - mx) / 9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]] valid_labels\n",
      "float32 valid_labels type\n",
      "(823, 10) valid_labels.shape\n",
      "[[ 0.32545128  0.00920499  0.00777778 ...,  0.00777778  0.00777778\n",
      "   0.61253059]\n",
      " [ 0.67435294  0.00777778  0.00777778 ...,  0.00777778  0.00777778\n",
      "   0.28782073]\n",
      " [ 0.62463993  0.00777778  0.00777778 ...,  0.00777778  0.00777778\n",
      "   0.28785765]\n",
      " ..., \n",
      " [ 0.08136775  0.02805007  0.02607597 ...,  0.03632662  0.30010533\n",
      "   0.50303668]\n",
      " [ 0.33036822  0.03377842  0.01220943 ...,  0.00777778  0.07727412\n",
      "   0.47432873]\n",
      " [ 0.23242027  0.0451872   0.03887215 ...,  0.01095971  0.26165023\n",
      "   0.29060861]] valid_preds_c type\n",
      "float32 valid_preds_c\n",
      "(823, 10) valid_preds_c.shape\n",
      "0.876032 validation mean\n",
      "0.876032492352 score\n"
     ]
    }
   ],
   "source": [
    "valid_classes = valid_batches.classes\n",
    "valid_labels = to_categorical(valid_classes, 10).astype('float32')\n",
    "\n",
    "valid_preds_c = do_clip(valid_preds, 0.93)\n",
    "\n",
    "print(valid_labels, 'valid_labels')\n",
    "print(valid_labels.dtype, 'valid_labels type')\n",
    "print(valid_labels.shape, 'valid_labels.shape')\n",
    "\n",
    "print(valid_preds_c, 'valid_preds_c type')\n",
    "print(valid_preds_c.dtype, 'valid_preds_c')\n",
    "print(valid_preds_c.shape, 'valid_preds_c.shape')\n",
    "\n",
    "cc = categorical_crossentropy(K.constant(valid_labels), K.constant(valid_preds_c))\n",
    "valid_mean = np.mean(cc.eval(session=K.get_session()))\n",
    "\n",
    "score = log_loss(valid_labels, valid_preds_c)\n",
    "\n",
    "print(valid_mean, 'validation mean')\n",
    "print(score, 'score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04818407,  0.00777778,  0.01461318, ...,  0.00777778,\n",
       "         0.0098451 ,  0.00777778],\n",
       "       [ 0.89036   ,  0.00779618,  0.00777778, ...,  0.00777778,\n",
       "         0.00777778,  0.06173703],\n",
       "       [ 0.00777778,  0.0145776 ,  0.91266948, ...,  0.00777778,\n",
       "         0.02415638,  0.00777778],\n",
       "       ..., \n",
       "       [ 0.43349975,  0.3404859 ,  0.04322587, ...,  0.00777778,\n",
       "         0.02240543,  0.0698687 ],\n",
       "       [ 0.00777778,  0.01101675,  0.11367005, ...,  0.70606726,\n",
       "         0.11160305,  0.00823002],\n",
       "       [ 0.26121113,  0.05492519,  0.05338839, ...,  0.02645271,\n",
       "         0.10260747,  0.22411396]], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_c = do_clip(test_preds, 0.93)\n",
    "test_preds_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read model from cache and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "338/338 [==============================] - 183s - loss: 1.5169 - acc: 0.5037 - val_loss: 5.3738 - val_acc: 0.2224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbdcd7ace80>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = read_model(data_dir, 'architecture-2017-10-10-22.json', 'model-weights-2017-10-10-22.json')\n",
    "\n",
    "m1.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "m1.optimizer.lr = 0.008\n",
    "m1.fit_generator(t_batches, t_steps, epochs=1, validation_data=v_batches, validation_steps=v_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_31 (Batc (None, 224, 224, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 222, 222, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 72, 72, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 200)               7373000   \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 7,395,598\n",
      "Trainable params: 7,395,000\n",
      "Non-trainable params: 598\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
